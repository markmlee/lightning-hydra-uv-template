_target_: src.data.tinyshakespeare_datamodule.TinyShakespeareDataModule
tokenizer_name: "HuggingFaceTB/SmolLM-135M"
max_length: 256  # Maximum sequence length for each sample
batch_size: 8  # Batch size (will be divided by number of devices in DDP)
train_val_test_split: [0.9, 0.05, 0.05]  # 90% train, 5% val, 5% test
num_workers: 4  # Number of dataloader workers
pin_memory: True  # Pin memory for faster GPU transfer
persistent_workers: True  # Keep workers alive between epochs
